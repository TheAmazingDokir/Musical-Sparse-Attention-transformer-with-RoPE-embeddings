{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"kritanjalijain/maestropianomidi\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LQxDV8qf4Fut",
        "outputId": "682658d4-ea7b-4061-c21b-98eee5748e39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/kritanjalijain/maestropianomidi?dataset_version_number=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 55.8M/55.8M [00:00<00:00, 152MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/kritanjalijain/maestropianomidi/versions/1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-05-24T18:42:48.894251Z",
          "iopub.status.busy": "2025-05-24T18:42:48.893905Z",
          "iopub.status.idle": "2025-05-24T18:42:56.521802Z",
          "shell.execute_reply": "2025-05-24T18:42:56.520928Z",
          "shell.execute_reply.started": "2025-05-24T18:42:48.894222Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WUjol3aF3lI1",
        "outputId": "ae7a1a73-f5ed-4e2b-e6e5-bb0a6cdc2df1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mido\n",
            "  Downloading mido-1.3.3-py3-none-any.whl.metadata (6.4 kB)\n",
            "Collecting pretty_midi\n",
            "  Downloading pretty_midi-0.2.11.tar.gz (5.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m67.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting midi-neural-processor\n",
            "  Downloading midi_neural_processor-1.0.3-py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from mido) (25.0)\n",
            "Requirement already satisfied: numpy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from pretty_midi) (2.0.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from pretty_midi) (1.17.0)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.12/dist-packages (from pretty_midi) (6.5.2)\n",
            "Requirement already satisfied: setuptools>=43.0.0 in /usr/local/lib/python3.12/dist-packages (from midi-neural-processor) (75.2.0)\n",
            "Downloading mido-1.3.3-py3-none-any.whl (54 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading midi_neural_processor-1.0.3-py3-none-any.whl (5.6 kB)\n",
            "Building wheels for collected packages: pretty_midi\n",
            "  Building wheel for pretty_midi (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pretty_midi: filename=pretty_midi-0.2.11-py3-none-any.whl size=5595886 sha256=bc8a551086f8d237aad9d1a56d0217bc350f5de72eaabf909b1d663d3b7e14b5\n",
            "  Stored in directory: /root/.cache/pip/wheels/f4/ad/93/a7042fe12668827574927ade9deec7f29aad2a1001b1501882\n",
            "Successfully built pretty_midi\n",
            "Installing collected packages: mido, pretty_midi, midi-neural-processor\n",
            "Successfully installed midi-neural-processor-1.0.3 mido-1.3.3 pretty_midi-0.2.11\n"
          ]
        }
      ],
      "source": [
        "pip install mido pretty_midi midi-neural-processor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-05-24T18:43:28.639477Z",
          "iopub.status.busy": "2025-05-24T18:43:28.639008Z",
          "iopub.status.idle": "2025-05-24T18:43:28.643551Z",
          "shell.execute_reply": "2025-05-24T18:43:28.642637Z",
          "shell.execute_reply.started": "2025-05-24T18:43:28.639447Z"
        },
        "trusted": true,
        "id": "OUYZPYIs3lI4"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import random\n",
        "import numpy as np\n",
        "import midi_neural_processor.processor as midi_tokenizer\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-05-24T18:43:30.317409Z",
          "iopub.status.busy": "2025-05-24T18:43:30.317114Z",
          "iopub.status.idle": "2025-05-24T18:43:30.373502Z",
          "shell.execute_reply": "2025-05-24T18:43:30.372532Z",
          "shell.execute_reply.started": "2025-05-24T18:43:30.317385Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9HuIarut3lI5",
        "outputId": "17e3da8f-2ae0-4936-a8ff-40ed3cd7aaf9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7a37f3ef2cd0>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "#hyperparameters\n",
        "vocab_size = 512\n",
        "batch_size = 64 # N of independent sequneces processed in parallel\n",
        "block_size = 512 # the maximum context length for prediction\n",
        "max_iters = 10000\n",
        "eval_interval = 500\n",
        "learning_rate = 3e-4\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 768\n",
        "n_layer = 12\n",
        "n_head = 8\n",
        "dropout = 0.1\n",
        "\n",
        "# Sparse attention parameters for MIDI\n",
        "window_size = 32  # Local sliding window size\n",
        "stride_size = 4   # Strided attention every 4 positions (beat level)\n",
        "num_global_tokens = 4  # Number of global attention positions\n",
        "\n",
        "print(device)\n",
        "#----------------------------------------------------------\n",
        "\n",
        "torch.manual_seed(555)\n",
        "\n",
        "#----------------------------------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-05-24T18:43:35.168979Z",
          "iopub.status.busy": "2025-05-24T18:43:35.168650Z",
          "iopub.status.idle": "2025-05-24T18:48:11.724058Z",
          "shell.execute_reply": "2025-05-24T18:48:11.722943Z",
          "shell.execute_reply.started": "2025-05-24T18:43:35.168950Z"
        },
        "trusted": true,
        "id": "A_laNl9O3lI6"
      },
      "outputs": [],
      "source": [
        "folder_path = '/kaggle/input/maestropianomidi'\n",
        "\n",
        "data = []\n",
        "# Recursively walk through all subdirectories\n",
        "for root, dirs, files in os.walk(folder_path):\n",
        "    for file in files:\n",
        "        full_path = os.path.join(root, file)\n",
        "        try:\n",
        "            tokens_cur = midi_tokenizer.encode_midi(full_path)\n",
        "            if len(tokens_cur) > 1000:\n",
        "                tokens_cur = torch.tensor(tokens_cur)\n",
        "                data.append(tokens_cur)\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "# Split to train and val\n",
        "n = int(0.9*len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# data loading\n",
        "def get_batch(split):\n",
        "    #generate a small batch of data of inputs\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    index = random.randint(0, len(data)-1)\n",
        "    data = data[index]\n",
        "    ix = torch.randint(len(data)-block_size, (batch_size, ))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-05-24T18:49:30.683596Z",
          "iopub.status.busy": "2025-05-24T18:49:30.683304Z",
          "iopub.status.idle": "2025-05-24T18:49:30.688576Z",
          "shell.execute_reply": "2025-05-24T18:49:30.687790Z",
          "shell.execute_reply.started": "2025-05-24T18:49:30.683572Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TvyMXN_U3lI6",
        "outputId": "0348c08e-f85e-43d5-a9f0-5f39f4585c45"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1145"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "len(train_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hpw_FSpY3lI7"
      },
      "outputs": [],
      "source": [
        "def create_sparse_attention_mask(seq_len, window_size=32, stride_size=4, num_global=4):\n",
        "    \"\"\"\n",
        "    Create sparse attention mask for MIDI generation:\n",
        "    - Local sliding window for note-to-note relationships\n",
        "    - Strided pattern for beat/measure relationships\n",
        "    - Global tokens for long-range dependencies\n",
        "    \"\"\"\n",
        "    mask = torch.zeros(seq_len, seq_len, dtype=torch.bool)\n",
        "\n",
        "    for i in range(seq_len):\n",
        "        # 1. Local sliding window (±window_size positions)\n",
        "        start = max(0, i - window_size)\n",
        "        end = min(seq_len, i + window_size + 1)\n",
        "        mask[i, start:end] = True\n",
        "\n",
        "        # 2. Strided attention (every stride_size positions)\n",
        "        strided_positions = torch.arange(0, seq_len, stride_size)\n",
        "        mask[i, strided_positions] = True\n",
        "\n",
        "        # 3. Global tokens (first few positions attend to/from everything)\n",
        "        if i < num_global:\n",
        "            mask[i, :] = True  # Global tokens attend to everything\n",
        "        mask[:, i] = mask[:, i] | (i < num_global)  # Everything attends to global tokens\n",
        "\n",
        "    # Ensure causal masking (no future attention)\n",
        "    causal_mask = torch.tril(torch.ones(seq_len, seq_len, dtype=torch.bool))\n",
        "    mask = mask & causal_mask\n",
        "\n",
        "    return mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qTJmYf7U3lI7"
      },
      "outputs": [],
      "source": [
        "# Rotary Position Embedding (RoPE) Implementation\n",
        "def create_rope_cache(seq_len, dim, theta=10000.0, device='cpu'):\n",
        "    \"\"\"\n",
        "    Create RoPE (Rotary Position Embedding) cache for efficient computation\n",
        "    \"\"\"\n",
        "    # Create position indices\n",
        "    pos = torch.arange(seq_len, device=device, dtype=torch.float32)\n",
        "\n",
        "    # Create frequency tensor\n",
        "    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2, device=device, dtype=torch.float32) / dim))\n",
        "\n",
        "    # Create frequency matrix: pos x freqs\n",
        "    freqs = torch.outer(pos, freqs)  # (seq_len, dim//2)\n",
        "\n",
        "    # Create cos and sin components\n",
        "    cos_cached = torch.cos(freqs)  # (seq_len, dim//2)\n",
        "    sin_cached = torch.sin(freqs)  # (seq_len, dim//2)\n",
        "\n",
        "    return cos_cached, sin_cached\n",
        "\n",
        "def apply_rope(x, cos_cached, sin_cached):\n",
        "    \"\"\"\n",
        "    Apply rotary position embedding to input tensor x\n",
        "    x: (batch_size, seq_len, n_heads, head_dim) or (batch_size, seq_len, head_dim)\n",
        "    \"\"\"\n",
        "    *batch_dims, seq_len, d = x.shape\n",
        "\n",
        "    # Ensure we don't exceed cache length\n",
        "    seq_len = min(seq_len, cos_cached.shape[0])\n",
        "\n",
        "    # Get the cos/sin values for this sequence length\n",
        "    cos = cos_cached[:seq_len]  # (seq_len, d//2)\n",
        "    sin = sin_cached[:seq_len]  # (seq_len, d//2)\n",
        "\n",
        "    # Reshape x to separate even/odd dimensions\n",
        "    x1 = x[..., ::2]   # Even indices: (batch_dims, seq_len, d//2)\n",
        "    x2 = x[..., 1::2]  # Odd indices: (batch_dims, seq_len, d//2)\n",
        "\n",
        "    # Apply rotation\n",
        "    # Expand cos/sin to match x dimensions\n",
        "    cos_expanded = cos.view(*([1] * len(batch_dims)), seq_len, -1)\n",
        "    sin_expanded = sin.view(*([1] * len(batch_dims)), seq_len, -1)\n",
        "\n",
        "    # Rotary transformation\n",
        "    rotated_x1 = x1 * cos_expanded - x2 * sin_expanded\n",
        "    rotated_x2 = x1 * sin_expanded + x2 * cos_expanded\n",
        "\n",
        "    # Interleave back to original format\n",
        "    rotated = torch.stack([rotated_x1, rotated_x2], dim=-1)\n",
        "    rotated = rotated.flatten(start_dim=-2)  # Merge last two dims\n",
        "\n",
        "    return rotated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-05-24T18:49:33.115828Z",
          "iopub.status.busy": "2025-05-24T18:49:33.115537Z",
          "iopub.status.idle": "2025-05-24T18:49:33.129900Z",
          "shell.execute_reply": "2025-05-24T18:49:33.128956Z",
          "shell.execute_reply.started": "2025-05-24T18:49:33.115806Z"
        },
        "trusted": true,
        "id": "7_1revjF3lI8"
      },
      "outputs": [],
      "source": [
        "class SparseRoPEHead(nn.Module):\n",
        "    \"\"\"Sparse attention head with RoPE (Rotary Position Embedding) for MIDI generation\"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.head_size = head_size\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "\n",
        "        # Initialize mask cache as regular attribute\n",
        "        self._mask_cache = {}\n",
        "\n",
        "        # Initialize RoPE cache\n",
        "        self._rope_cos_cache = None\n",
        "        self._rope_sin_cache = None\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def get_sparse_mask(self, seq_len):\n",
        "        \"\"\"Get or create sparse attention mask for given sequence length\"\"\"\n",
        "        if seq_len not in self._mask_cache:\n",
        "            mask = create_sparse_attention_mask(\n",
        "                seq_len,\n",
        "                window_size=window_size,\n",
        "                stride_size=stride_size,\n",
        "                num_global=num_global_tokens\n",
        "            )\n",
        "            # Store on correct device\n",
        "            mask = mask.to(self.key.weight.device)\n",
        "            # Cache the mask\n",
        "            self._mask_cache[seq_len] = mask\n",
        "\n",
        "        return self._mask_cache[seq_len]\n",
        "\n",
        "    def get_rope_cache(self, seq_len):\n",
        "        \"\"\"Get or create RoPE cache for given sequence length\"\"\"\n",
        "        if (self._rope_cos_cache is None or\n",
        "            self._rope_sin_cache is None or\n",
        "            self._rope_cos_cache.shape[0] < seq_len):\n",
        "\n",
        "            cos_cached, sin_cached = create_rope_cache(\n",
        "                max(seq_len, 512),  # Cache a bit more for efficiency\n",
        "                self.head_size,\n",
        "                device=self.key.weight.device\n",
        "            )\n",
        "            self._rope_cos_cache = cos_cached\n",
        "            self._rope_sin_cache = sin_cached\n",
        "\n",
        "        return self._rope_cos_cache, self._rope_sin_cache\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.shape\n",
        "        k = self.key(x)  # (B, T, head_size)\n",
        "        q = self.query(x)  # (B, T, head_size)\n",
        "        v = self.value(x)  # (B, T, head_size)\n",
        "\n",
        "        # Apply RoPE to queries and keys\n",
        "        cos_cached, sin_cached = self.get_rope_cache(T)\n",
        "        q = apply_rope(q, cos_cached, sin_cached)\n",
        "        k = apply_rope(k, cos_cached, sin_cached)\n",
        "\n",
        "        # Compute attention scores\n",
        "        wei = q @ k.transpose(-2, -1) * (self.head_size ** -0.5)  # (B, T, T)\n",
        "\n",
        "        # Apply sparse attention mask\n",
        "        sparse_mask = self.get_sparse_mask(T)\n",
        "        wei = wei.masked_fill(~sparse_mask, float('-inf'))\n",
        "\n",
        "        # Softmax and dropout\n",
        "        wei = F.softmax(wei, dim=-1)\n",
        "        wei = self.dropout(wei)\n",
        "\n",
        "        # Weighted aggregation (no RoPE needed for values)\n",
        "        out = wei @ v\n",
        "        return out\n",
        "\n",
        "class SparseHead(nn.Module):\n",
        "    \"\"\"Original Sparse attention head (without RoPE) for comparison\"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.head_size = head_size\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "\n",
        "        # Initialize mask cache as regular attribute\n",
        "        self._mask_cache = {}\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def get_sparse_mask(self, seq_len):\n",
        "        \"\"\"Get or create sparse attention mask for given sequence length\"\"\"\n",
        "        if seq_len not in self._mask_cache:\n",
        "            mask = create_sparse_attention_mask(\n",
        "                seq_len,\n",
        "                window_size=window_size,\n",
        "                stride_size=stride_size,\n",
        "                num_global=num_global_tokens\n",
        "            )\n",
        "            # Store on correct device\n",
        "            mask = mask.to(self.key.weight.device)\n",
        "            # Cache the mask\n",
        "            self._mask_cache[seq_len] = mask\n",
        "\n",
        "        return self._mask_cache[seq_len]\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.shape\n",
        "        k = self.key(x)  # (B, T, head_size)\n",
        "        q = self.query(x)  # (B, T, head_size)\n",
        "        v = self.value(x)  # (B, T, head_size)\n",
        "\n",
        "        # Compute attention scores\n",
        "        wei = q @ k.transpose(-2, -1) * (self.head_size ** -0.5)  # (B, T, T)\n",
        "\n",
        "        # Apply sparse attention mask\n",
        "        sparse_mask = self.get_sparse_mask(T)\n",
        "        wei = wei.masked_fill(~sparse_mask, float('-inf'))\n",
        "\n",
        "        # Softmax and dropout\n",
        "        wei = F.softmax(wei, dim=-1)\n",
        "        wei = self.dropout(wei)\n",
        "\n",
        "        # Weighted aggregation\n",
        "        out = wei @ v\n",
        "        return out\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\"Original dense attention head for comparison\"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.shape\n",
        "        k = self.key(x) # (B, T, 16)\n",
        "        q = self.query(x) # (B, T, 16)\n",
        "\n",
        "        # compute attention scores\n",
        "        wei = q @ k.transpose(-2, -1) * C**-0.5 # (B, T, 16) @ (B, 16, T) ---> (B, T, T)\n",
        "        # Remove future token to not communicate with them\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
        "        # Softmax to get values that sum up to 1 - normalization\n",
        "        wei = F.softmax(wei, dim=-1)\n",
        "        wei = self.dropout(wei)\n",
        "\n",
        "        # perfrom weighted aggregation of values\n",
        "        v = self.value(x)\n",
        "        out = wei @ v\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, num_heads, head_size, use_sparse=True, use_rope=True):\n",
        "        super().__init__()\n",
        "\n",
        "        # Choose attention head type based on configuration\n",
        "        if use_sparse and use_rope:\n",
        "            HeadClass = SparseRoPEHead\n",
        "        elif use_sparse:\n",
        "            HeadClass = SparseHead\n",
        "        else:\n",
        "            HeadClass = Head\n",
        "\n",
        "        self.heads = nn.ModuleList([HeadClass(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    \"\"\" Simple layer followed by non-linearity\"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self,x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation\"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head, use_sparse=True, use_rope=True):\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size, use_sparse=use_sparse, use_rope=use_rope)\n",
        "        self.ffwd = FeedForward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "# Transformer Model with RoPE\n",
        "class TransformerModel(nn.Module):\n",
        "\n",
        "    def __init__(self, use_sparse=True, use_rope=True):\n",
        "        super().__init__()\n",
        "        self.use_rope = use_rope\n",
        "\n",
        "        # Token embeddings\n",
        "        self.token_enbedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "\n",
        "        # Positional embeddings (only used if not using RoPE)\n",
        "        if not use_rope:\n",
        "            self.positiion_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        else:\n",
        "            self.positiion_embedding_table = None\n",
        "\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head, use_sparse=use_sparse, use_rope=use_rope) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # Final norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # Token embeddings\n",
        "        token_emb = self.token_enbedding_table(idx) # (B,T,C)\n",
        "\n",
        "        # Add positional embeddings only if not using RoPE\n",
        "        if self.use_rope:\n",
        "            x = token_emb  # RoPE handles position encoding in attention\n",
        "        else:\n",
        "            pos_emb = self.positiion_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "            x = token_emb + pos_emb # (B,T,C)\n",
        "\n",
        "        x = self.blocks(x) # apply self attention\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.lm_head(x) # (B, T, vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx=None, max_new_tokens=256, temperature=1.0, top_k=None, top_p=0.9):\n",
        "          \"\"\"\n",
        "          Generate MIDI tokens with improved sampling strategies\n",
        "\n",
        "          Args:\n",
        "              idx: Initial sequence (optional). If None, starts with common MIDI start tokens\n",
        "              max_new_tokens: Number of tokens to generate\n",
        "              temperature: Sampling temperature (higher = more random)\n",
        "              top_k: Keep only top k tokens for sampling\n",
        "              top_p: Nucleus sampling threshold\n",
        "          \"\"\"\n",
        "          # Smart initialization if no initial sequence provided\n",
        "          if idx is None:\n",
        "              # Start with common MIDI beginning tokens\n",
        "              # These typically include tempo, time signature, and initial note events\n",
        "              start_tokens = [\n",
        "                  1,   # Start of sequence token (if your tokenizer uses this)\n",
        "                  64,  # Common tempo token\n",
        "                  32,  # Time signature token\n",
        "                  60,  # Middle C note\n",
        "              ]\n",
        "              # Pad with a few more reasonable starting tokens\n",
        "              start_tokens.extend([65, 67, 69])  # C major chord notes\n",
        "\n",
        "              idx = torch.tensor(start_tokens, device=device).unsqueeze(0)  # (1, start_length)\n",
        "\n",
        "          # Ensure idx is on the correct device and has batch dimension\n",
        "          if idx.dim() == 1:\n",
        "              idx = idx.unsqueeze(0)\n",
        "          idx = idx.to(device)\n",
        "\n",
        "          self.eval()  # Set to evaluation mode\n",
        "\n",
        "          with torch.no_grad():\n",
        "              for i in range(max_new_tokens):\n",
        "                  # Crop context if it exceeds block_size\n",
        "                  idx_cond = idx[:, -block_size:]\n",
        "\n",
        "                  # Get predictions\n",
        "                  logits, _ = self(idx_cond)\n",
        "\n",
        "                  # Focus only on the last time step\n",
        "                  logits = logits[:, -1, :] / temperature  # (B, C)\n",
        "\n",
        "                  # Apply top-k filtering\n",
        "                  if top_k is not None:\n",
        "                      v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
        "                      logits[logits < v[:, [-1]]] = -float('inf')\n",
        "\n",
        "                  # Apply top-p (nucleus) filtering\n",
        "                  if top_p < 1.0:\n",
        "                      sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
        "                      cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
        "\n",
        "                      # Remove tokens with cumulative probability above the threshold\n",
        "                      sorted_indices_to_remove = cumulative_probs > top_p\n",
        "                      # Shift the indices to the right to keep also the first token above the threshold\n",
        "                      sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
        "                      sorted_indices_to_remove[..., 0] = 0\n",
        "\n",
        "                      # Scatter sorted tensors to original indexing\n",
        "                      indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)\n",
        "                      logits[indices_to_remove] = -float('inf')\n",
        "\n",
        "                  # Convert to probabilities\n",
        "                  probs = F.softmax(logits, dim=-1)\n",
        "\n",
        "                  # Sample from the distribution\n",
        "                  idx_next = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "                  # Append to the sequence\n",
        "                  idx = torch.cat((idx, idx_next), dim=1)\n",
        "\n",
        "                  # Optional: Add some musical structure by encouraging certain patterns\n",
        "                  # You could add logic here to bias towards musically coherent sequences\n",
        "\n",
        "          self.train()  # Return to training mode\n",
        "          return idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-05-24T18:50:04.911506Z",
          "iopub.status.busy": "2025-05-24T18:50:04.911204Z",
          "iopub.status.idle": "2025-05-24T18:50:06.561632Z",
          "shell.execute_reply": "2025-05-24T18:50:06.560718Z",
          "shell.execute_reply.started": "2025-05-24T18:50:04.911484Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NquZiOs63lI9",
        "outputId": "9ecaae3a-d359-4a19-e1d1-439f1d71ed09"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total trainable parameters: 85,815,296\n"
          ]
        }
      ],
      "source": [
        "# Create model with sparse attention\n",
        "model = TransformerModel(use_sparse=True, use_rope=True)\n",
        "m = model.to(device)\n",
        "best_metric = float('inf')  # Initialize with a large value for loss, or -inf for accuracy\n",
        "best_model_state = None\n",
        "\n",
        "# Add optimizer\n",
        "optimizer = torch.optim.AdamW(m.parameters(), lr=learning_rate, weight_decay=0.001)\n",
        "scaler = torch.GradScaler()\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
        "    optimizer,\n",
        "    T_0=1000,\n",
        "    T_mult=2,\n",
        "    eta_min=1e-6\n",
        ")\n",
        "\n",
        "# Count parameters\n",
        "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"Total trainable parameters: {total_params:,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-22T02:25:40.915319Z",
          "iopub.status.busy": "2025-02-22T02:25:40.914913Z",
          "iopub.status.idle": "2025-02-22T04:14:26.407460Z",
          "shell.execute_reply": "2025-02-22T04:14:26.406770Z",
          "shell.execute_reply.started": "2025-02-22T02:25:40.915294Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mdj8NtL83lI_",
        "outputId": "03f2535a-571b-41f8-8f52-36cf90ab88a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 6.3803, val loss 6.3864\n",
            "step 500: train loss 3.6963, val loss 3.7382\n",
            "step 1000: train loss 3.2139, val loss 3.2182\n",
            "step 1500: train loss 2.8565, val loss 2.8594\n",
            "step 2000: train loss 2.5657, val loss 2.6215\n",
            "step 2500: train loss 2.4466, val loss 2.5107\n",
            "step 3000: train loss 2.3973, val loss 2.4850\n",
            "step 3500: train loss 2.4105, val loss 2.4634\n",
            "step 4000: train loss 2.3303, val loss 2.4135\n",
            "step 4500: train loss 2.2319, val loss 2.3577\n",
            "step 5000: train loss 2.1753, val loss 2.3039\n",
            "step 5500: train loss 2.1036, val loss 2.2530\n",
            "step 6000: train loss 2.0514, val loss 2.2427\n",
            "step 6500: train loss 2.0422, val loss 2.2109\n",
            "step 7000: train loss 2.0420, val loss 2.2050\n",
            "step 7500: train loss 2.1124, val loss 2.2887\n",
            "step 8000: train loss 2.1193, val loss 2.2679\n",
            "step 8500: train loss 2.0359, val loss 2.2173\n",
            "step 9000: train loss 2.0522, val loss 2.2558\n",
            "step 9500: train loss 1.9937, val loss 2.1891\n"
          ]
        }
      ],
      "source": [
        "for iter in range(max_iters):\n",
        "\n",
        "    if iter % eval_interval == 0:\n",
        "        losses = estimate_loss()\n",
        "        if losses['val'] < best_metric:  # Update condition based on your metric and task\n",
        "            best_metric = losses['val']\n",
        "            best_model_state = model.state_dict().copy()  # Save the model state\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    xb, yb = get_batch('train')\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    with torch.autocast(device_type=\"cuda\", dtype=torch.bfloat16):\n",
        "        logits, loss = m(xb, yb)\n",
        "\n",
        "    scaler.scale(loss).backward()\n",
        "    scaler.step(optimizer)\n",
        "    scaler.update()\n",
        "\n",
        "    scheduler.step()\n",
        "#-----------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-22T04:36:29.319290Z",
          "iopub.status.busy": "2025-02-22T04:36:29.318945Z",
          "iopub.status.idle": "2025-02-22T04:36:29.679884Z",
          "shell.execute_reply": "2025-02-22T04:36:29.678979Z",
          "shell.execute_reply.started": "2025-02-22T04:36:29.319260Z"
        },
        "trusted": true,
        "id": "eTEDAQXK3lI_"
      },
      "outputs": [],
      "source": [
        "torch.save(best_model_state, 'best-midi-classical-sound.pth')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load(\"/content/best-midi-classical-sound.pth\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sldo82P67BOk",
        "outputId": "081fd274-e107-4ab5-9c65-a43deebff882"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-05-24T19:05:39.252380Z",
          "iopub.status.busy": "2025-05-24T19:05:39.252095Z",
          "iopub.status.idle": "2025-05-24T19:06:07.164534Z",
          "shell.execute_reply": "2025-05-24T19:06:07.163556Z",
          "shell.execute_reply.started": "2025-05-24T19:05:39.252357Z"
        },
        "trusted": true,
        "id": "G1tnDEtI3lI_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12347b75-6a92-4e58-fe04-f882a9e5699c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pretty_midi.pretty_midi.PrettyMIDI at 0x7ab271ae8d10>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "start = val_data[0][:256].unsqueeze(0).to(device)\n",
        "tokens = model.generate(start, 256)\n",
        "tokens = torch.clamp(tokens, max=383)\n",
        "\n",
        "tokens = tokens.squeeze(0).tolist()\n",
        "midi_tokenizer.decode_midi(tokens, 'output.mid')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = model.generate(max_new_tokens=512, temperature=1.2, top_k=80, top_p=0.95)"
      ],
      "metadata": {
        "id": "s4WDzhKE7byy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = torch.clamp(tokens, max=383)\n",
        "tokens = tokens.squeeze(0).tolist()\n",
        "midi_tokenizer.decode_midi(tokens, 'output.mid')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QTXK53paAbHc",
        "outputId": "7f0fb4fa-1a26-4976-fde5-b893f92191ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "info removed pitch: 84\n",
            "info removed pitch: 41\n",
            "info removed pitch: 59\n",
            "info removed pitch: 52\n",
            "info removed pitch: 81\n",
            "info removed pitch: 76\n",
            "info removed pitch: 77\n",
            "info removed pitch: 77\n",
            "info removed pitch: 73\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pretty_midi.pretty_midi.PrettyMIDI at 0x7a37f2c64590>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    }
  ],
  "metadata": {
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "datasetId": 1304058,
          "sourceId": 2172408,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30822,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "gpuType": "L4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}